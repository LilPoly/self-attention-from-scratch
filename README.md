# Self-Attention from Scratch

## Overview
This repository implements the Self-Attention mechanism from scratch using Python and NumPy. It demonstrates the core principles of the self-attention mechanism, which is a fundamental part of transformer models, such as BERT and GPT.

## Requirements
- Python 3.13
- NumPy
To install the necessary dependencies, use the following command:
``` python
pip install -r requirements.txt
```
Note: If requirements.txt is not provided, make sure to install NumPy manually via:
``` python
pip install numpy
```
## Usage
1. Clone the repository
``` python
git clone https://github.com/LilPoly/self-attention-from-scratch
cd self-attention-from-scratch
```
2. Run the code
To run the self-attention code, simply execute the main Python script:
``` python
python self_attention.py
```


